---
title: "p8105_hw3_wq2161"
author: "Wanxin Qi"
date: "10/16/2021"
output: github_document
---

```{r setup, echo = FALSE, message = FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```


## Problem 1

Load the *instacart* data from the *p8105.datasets*.

```{r instacart}
data("instacart")
instacart = instacart %>%
  janitor::clean_names()
```

* Description of the *instacart* dataset

The instacart dataset includes information on some of the items ordered from instacart online grocery website. It contains `r ncol(instacart)` variables which are `r colnames(instacart)`. Specifically, order_dow is the day of the week on which the order was placed, and order_hour_of_day is the hour of the day on which the order was placed. There are `r nrow(instacart)` items where each row of the dataset is an ordered item. 

For example, the first item was ordered by the user with user id `r pull(instacart[1,], user_id)`, and the order id is `r pull(instacart[1,], order_id)`. It is a `r pull(instacart[1,], product_name)` with product id `r pull(instacart[1,], product_id)`, which belongs to `r pull(instacart[1,], aisle)` aisle with aisle id `r pull(instacart[1,], aisle_id)` and the department of `r pull(instacart[1,], department)` with department id `r pull(instacart[1,], department_id)`. The order in which this item was added to cart was `r pull(instacart[1,], add_to_cart_order)`, and this product has been ordered by this user in the past since reorder has value of `r pull(instacart[1,], reordered)`. This order is the `r pull(instacart[1,], order_number)` order for this user. It was placed on the `r pull(instacart[1,], order_hour_of_day)` hour of the day and the `r pull(instacart[1,], order_dow)` day of the week. The days since the last order for the user is `r pull(instacart[1,], days_since_prior_order)`. It belongs to the "train" evaluation set.

* How many aisles are there, and which aisles are the most items ordered from?

```{r aisle_num}
aisle_num = instacart %>%
  group_by(aisle) %>%
  summarize(n_aisle = n())
aisle_num

most_items = aisle_num %>%
  mutate(aisle_rank = min_rank(desc(n_aisle))) %>%
  filter(aisle_rank == 1)
most_items
```

There are `r nrow(aisle_num)` aisles. The aisle that the most items ordered from is `r pull(most_items, aisle)`.

* Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r aisle_num_plot}
aisle_num %>%
  filter(n_aisle > 10000) %>%
  mutate(aisle = fct_reorder(aisle, n_aisle)) %>%
  ggplot(aes(x = n_aisle, y = aisle)) +
  geom_point(alpha = .5, size = .8) +
  labs(
    title = "The Number of Items Ordered in Each Aisle",
    subtitle = "Aisles with more than 10,000 items ordered",
    x = "Number of Items Ordered",
    y = "Aisle",
    caption = "Data from instacart"
  ) +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 5)) +
  scale_x_continuous(
    breaks = c(20000, 40000, 60000, 80000, 100000, 120000, 140000),
    labels = c("20k", "40k", "60k", "80k", "100k", "120k", "140k")
  )
```

The plot shows that the number of items ordered in fresh vegetables and fresh fruits are much more than other aisles. Most of the aisles have number of items ordered under 40,000.

* Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r product_num, message = FALSE}
product_num = instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>%
  summarize(n_product = n()) %>%
  mutate(product_rank = min_rank(desc(n_product))) %>%
  filter(product_rank <= 3) %>%
  arrange(aisle, product_rank)

product_num %>%
  knitr::kable()
```

Based on the table, although these products are the three most popular items of their aisles, the number they were ordered are different. The number of products purchased in baking ingredients is in hundreds, the number of products purchased in dog food care is in tens, and the number of products purchased in packaged vegetables fruits is in thousands. It is obvious that people all need vegetables and fruits for living.

* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r mean_hour, message = FALSE}
mean_hour = instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(
    mean = mean(order_hour_of_day)
  ) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean,
  )

mean_hour %>% knitr::kable(digit = 2)
```

The table shows the mean hour of the day at which `r pull(mean_hour, product_name)` are ordered on each day of the week. There are 7 days in a week range from 0 to 6. The mean hour of the day are all after 10 hours before 16 hours, indicating that the average time for people to shop is around afternoon.


## Problem 2

Load the *BRFSS* data from the *p8105.datasets* and clean.

```{r brfss}
data("brfss_smart2010")

brfss = brfss_smart2010 %>%
  janitor::clean_names() %>%
  rename(
    state = locationabbr,
    location = locationdesc,
  ) %>%
  filter(
    topic == "Overall Health",
    response %in% c("Poor", "Fair", "Good", "Very good", "Excellent")
    ) %>%
  mutate(
    response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))
  )
brfss
```

* In 2002, which states were observed at 7 or more locations? What about in 2010?

```{r state_year}
state_2002 = brfss %>%
  filter(year == 2002) %>%
  group_by(state) %>%
  summarize(n_location = n_distinct(location)) %>%
  filter(n_location >= 7)
state_2002

state_2010 = brfss %>%
  filter(year == 2010) %>%
  group_by(state) %>%
  summarize(n_location = n_distinct(location)) %>%
  filter(n_location >= 7)
state_2010
```

In 2002, there were `r nrow(state_2002)` states which were observed at 7 or more locations, which were `r pull(state_2002, state)`. In 2010, there were `r nrow(state_2010)` states which were observed at 7 or more locations, which were `r pull(state_2010, state)`.

* Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state.

```{r excellent_df_plot, message = FALSE, warning = FALSE}
excellent_df = brfss %>%
  filter(response == "Excellent") %>%
  group_by(year, state) %>%
  summarize(mean_data_value = mean(data_value))
excellent_df

excellent_df %>%
  ggplot(aes(x = year, y = mean_data_value)) +
  geom_line(aes(color = state), alpha = .5) +
  labs(
    title = "Average Data Value vs. Year for Each State",
    subtitle = "Only for Excellent Responses",
    x = "Year",
    y = "Average Data Value",
    caption = "Data from brfss_smart2010 dataset"
  ) +
  theme_minimal() +
  theme(legend.position = "right", legend.key.size = unit(0.02, "cm"))
```

Based on the plot, most of the average data value range from 15 to 30. The general trend of the average data value for states is decreasing with time. While there are some states whose average data value is fluctuated up and down with time.

* Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r NY_df, message = FALSE}
NY_df = brfss %>%
  filter(
    year %in% c(2006, 2010),
    state == "NY"
  ) %>%
  group_by(year,location, response) %>%
  summarize(data_value)
NY_df

NY_df %>%
  ggplot(aes(x = data_value, color = year)) +
  geom_density(alpha = 0.5) +
  facet_grid(. ~ year) +
  labs(
    title = "The Distribution of Data Value",
    subtitle = "For All Responses in NY in Year 2006 and 2010",
    x = "Data Value",
    y = "Density of Data Value",
    caption = "Data from brfss_smart2010 dataset"
  ) +
  theme(legend.position = "none")
```

(Comment)

## Problem 3

* Load, tidy, and otherwise wrangle the data.

```{r accel_data, message = FALSE}
accel_data = read_csv("data/accel_data.csv")

accel_data = accel_data %>%
  janitor::clean_names() %>%
  mutate(day_end = ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday")) %>%
  relocate(week, day_id, day, day_end) %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_min",
    values_to = "data_value"
  ) %>%
  separate(activity_min, into = c("activity", "minute")) %>%
  select(-activity) %>%
  mutate(
    week = as.numeric(week),
    day_id = as.numeric(day_id),
    day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
    day_end = as.character(day_end),
    minute = as.numeric(minute),
    data_value = as.numeric(data_value)
  )

accel_data
```

The dataset is a record of 5 weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). The dataset includes `r ncol(accel_data)` variables, which are `r colnames(accel_data)`, and `r nrow(accel_data)` observations. The first 5 variables record the specific time of an activity. For example, the first observation is on the 1st day of the 1st week, which is a Friday (a weekday), at 0:01 at midnight. The variable data_value record the data of that activity.

* Aggregate across minutes to create a total activity variable for each day, and create a table showing these totals.

```{r accel_total, message = FALSE}
accel_total = accel_data %>%
  group_by(week, day) %>%
  summarize(total_activity = sum(data_value)) %>%
  pivot_wider(
    names_from = day,
    values_from = total_activity
  )

accel_total %>%
  knitr::kable()
```

It doesn't show an apparent trend for the data, but generally, the data on weekend is less than weekday, especially for week 3 to 5.

* Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.

```{r 24_hour_plot, message = FALSE}
accel_data %>%
  ggplot(aes(x = minute, y = data_value, color = day)) +
  geom_point(alpha = 0.3, size = 0.5) +
  geom_smooth(se = FALSE, size = 0.7) + 
  labs(
    title = "The Data Value of the 24-Hour Activity Time Courses",
    subtitle = "For Each Day of the Week",
    x = "Activity Time",
    y = "Data Value",
    caption = "Data from Accel_data dataset"
  ) +
  scale_x_continuous(
    breaks = c(0, 360, 720, 1080, 1440),
    labels = c("12AM", "6AM", "12PM", "6PM", "12AM")
  ) +
  theme_minimal() +
  scale_color_viridis_d()
```

Based on the plot, the activity between 12AM and 6AM was low and stable, and it was increasing by time in a low pace for all the seven days in the week. The activity after 6AM are higher and different between days, especially for Friday to Sunday and the other four days.