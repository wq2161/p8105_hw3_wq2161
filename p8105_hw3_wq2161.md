p8105\_hw3\_wq2161
================
Wanxin Qi
10/16/2021

## Problem 1

Load the *instacart* data from the *p8105.datasets*.

``` r
data("instacart")
instacart = instacart %>%
  janitor::clean_names()
```

-   Description of the *instacart* dataset

The instacart dataset includes information on some of the items ordered
from instacart online grocery website. It contains 15 variables which
are order\_id, product\_id, add\_to\_cart\_order, reordered, user\_id,
eval\_set, order\_number, order\_dow, order\_hour\_of\_day,
days\_since\_prior\_order, product\_name, aisle\_id, department\_id,
aisle, department. Specifically, order\_dow is the day of the week on
which the order was placed, and order\_hour\_of\_day is the hour of the
day on which the order was placed. There are 1384617 items where each
row of the dataset is an ordered item.

For example, the first item was ordered by the user with user id 112108,
and the order id is 1. It is a Bulgarian Yogurt with product id 49302,
which belongs to yogurt aisle with aisle id 120 and the department of
dairy eggs with department id 16. The order in which this item was added
to cart was 1, and this product has been ordered by this user in the
past since reorder has value of 1. This order is the 4 order for this
user. It was placed on the 10 hour of the day and the 4 day of the week.
The days since the last order for the user is 9. It belongs to the
“train” evaluation set.

-   How many aisles are there, and which aisles are the most items
    ordered from?

``` r
aisle_num = instacart %>%
  group_by(aisle) %>%
  summarize(n_aisle = n())
aisle_num
```

    ## # A tibble: 134 × 2
    ##    aisle                  n_aisle
    ##    <chr>                    <int>
    ##  1 air fresheners candles    1067
    ##  2 asian foods               7007
    ##  3 baby accessories           306
    ##  4 baby bath body care        328
    ##  5 baby food formula        13198
    ##  6 bakery desserts           1501
    ##  7 baking ingredients       13088
    ##  8 baking supplies decor     1094
    ##  9 beauty                     287
    ## 10 beers coolers             1839
    ## # … with 124 more rows

``` r
most_items = aisle_num %>%
  mutate(aisle_rank = min_rank(desc(n_aisle))) %>%
  filter(aisle_rank == 1)
most_items
```

    ## # A tibble: 1 × 3
    ##   aisle            n_aisle aisle_rank
    ##   <chr>              <int>      <int>
    ## 1 fresh vegetables  150609          1

There are 134 aisles. The aisle that the most items ordered from is
fresh vegetables.

-   Make a plot that shows the number of items ordered in each aisle,
    limiting this to aisles with more than 10000 items ordered. Arrange
    aisles sensibly, and organize your plot so others can read it.

``` r
aisle_num %>%
  filter(n_aisle > 10000) %>%
  mutate(aisle = fct_reorder(aisle, n_aisle)) %>%
  ggplot(aes(x = n_aisle, y = aisle)) +
  geom_point() +
  labs(
    title = "The Number of Items Ordered in Each Aisle",
    subtitle = "Aisles with more than 10,000 items ordered",
    x = "Number of Items Ordered",
    y = "Aisle",
    caption = "Data from instacart"
  ) +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 8)) +
  scale_x_continuous(
    breaks = c(20000, 40000, 60000, 80000, 100000, 120000, 140000),
    labels = c("20k", "40k", "60k", "80k", "100k", "120k", "140k")
  )
```

<img src="p8105_hw3_wq2161_files/figure-gfm/aisle_num_plot-1.png" width="90%" />

Comment

-   Make a table showing the three most popular items in each of the
    aisles “baking ingredients”, “dog food care”, and “packaged
    vegetables fruits”. Include the number of times each item is ordered
    in your table.

``` r
product_num = instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>%
  summarize(n_product = n()) %>%
  mutate(product_rank = min_rank(desc(n_product))) %>%
  filter(product_rank <= 3) %>%
  arrange(aisle, product_rank)

product_num %>%
  knitr::kable()
```

| aisle                      | product\_name                                 | n\_product | product\_rank |
|:---------------------------|:----------------------------------------------|-----------:|--------------:|
| baking ingredients         | Light Brown Sugar                             |        499 |             1 |
| baking ingredients         | Pure Baking Soda                              |        387 |             2 |
| baking ingredients         | Cane Sugar                                    |        336 |             3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |         30 |             1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |         28 |             2 |
| dog food care              | Small Dog Biscuits                            |         26 |             3 |
| packaged vegetables fruits | Organic Baby Spinach                          |       9784 |             1 |
| packaged vegetables fruits | Organic Raspberries                           |       5546 |             2 |
| packaged vegetables fruits | Organic Blueberries                           |       4966 |             3 |

Based on the table, although these products are the three most popular
items of their aisles, the number they were ordered are different. The
number of products purchased in baking ingredients is in hundreds, the
number of products purchased in dog food care is in tens, and the number
of products purchased in packaged vegetables fruits is in thousands. It
is obvious that people all need vegetables and fruits for living.

-   Make a table showing the mean hour of the day at which Pink Lady
    Apples and Coffee Ice Cream are ordered on each day of the week;
    format this table for human readers (i.e. produce a 2 x 7 table).

``` r
mean_hour = instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(
    mean = mean(order_hour_of_day)
  ) %>%
  mutate(
    order_dow = recode(order_dow,
                       `0` = "Sun",
                       `1` = "Mon",
                       `2` = "Tue",
                       `3` = "Wed",
                       `4` = "Thu",
                       `5` = "Fri",
                       `6` = "Sat"
    )
  ) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean,
  )

mean_hour %>% knitr::kable(digit = 2)
```

| product\_name    |   Sun |   Mon |   Tue |   Wed |   Thu |   Fri |   Sat |
|:-----------------|------:|------:|------:|------:|------:|------:|------:|
| Coffee Ice Cream | 13.77 | 14.32 | 15.38 | 15.32 | 15.22 | 12.26 | 13.83 |
| Pink Lady Apples | 13.44 | 11.36 | 11.70 | 14.25 | 11.55 | 12.78 | 11.94 |

The table shows the mean hour of the day at which Coffee Ice Cream, Pink
Lady Apples are ordered on each day of the week. The mean hour of the
day are all after 10 hours before 16 hours, indicating that the average
time for people to shop is around afternoon.

## Problem 2

Load the *BRFSS* data from the *p8105.datasets* and clean.

``` r
data("brfss_smart2010")

brfss = brfss_smart2010 %>%
  janitor::clean_names() %>%
  rename(
    state = locationabbr,
    location = locationdesc,
  ) %>%
  filter(
    topic == "Overall Health",
    response %in% c("Poor", "Fair", "Good", "Very good", "Excellent")
    ) %>%
  mutate(
    response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))
  )
brfss
```

    ## # A tibble: 10,625 × 23
    ##     year state location   class  topic question  response sample_size data_value
    ##    <int> <chr> <chr>      <chr>  <chr> <chr>     <fct>          <int>      <dbl>
    ##  1  2010 AL    AL - Jeff… Healt… Over… How is y… Excelle…          94       18.9
    ##  2  2010 AL    AL - Jeff… Healt… Over… How is y… Very go…         148       30  
    ##  3  2010 AL    AL - Jeff… Healt… Over… How is y… Good             208       33.1
    ##  4  2010 AL    AL - Jeff… Healt… Over… How is y… Fair             107       12.5
    ##  5  2010 AL    AL - Jeff… Healt… Over… How is y… Poor              45        5.5
    ##  6  2010 AL    AL - Mobi… Healt… Over… How is y… Excelle…          91       15.6
    ##  7  2010 AL    AL - Mobi… Healt… Over… How is y… Very go…         177       31.3
    ##  8  2010 AL    AL - Mobi… Healt… Over… How is y… Good             224       31.2
    ##  9  2010 AL    AL - Mobi… Healt… Over… How is y… Fair             120       15.5
    ## 10  2010 AL    AL - Mobi… Healt… Over… How is y… Poor              66        6.4
    ## # … with 10,615 more rows, and 14 more variables: confidence_limit_low <dbl>,
    ## #   confidence_limit_high <dbl>, display_order <int>, data_value_unit <chr>,
    ## #   data_value_type <chr>, data_value_footnote_symbol <chr>,
    ## #   data_value_footnote <chr>, data_source <chr>, class_id <chr>,
    ## #   topic_id <chr>, location_id <chr>, question_id <chr>, respid <chr>,
    ## #   geo_location <chr>

-   In 2002, which states were observed at 7 or more locations? What
    about in 2010?

``` r
state_2002 = brfss %>%
  filter(year == 2002) %>%
  group_by(state) %>%
  summarize(n_location = n()) %>%
  filter(n_location >= 7)
state_2002
```

    ## # A tibble: 36 × 2
    ##    state n_location
    ##    <chr>      <int>
    ##  1 AZ            10
    ##  2 CO            20
    ##  3 CT            35
    ##  4 DE            15
    ##  5 FL            35
    ##  6 GA            15
    ##  7 HI            20
    ##  8 ID            10
    ##  9 IL            15
    ## 10 IN            10
    ## # … with 26 more rows

``` r
state_2010 = brfss %>%
  filter(year == 2010) %>%
  group_by(state) %>%
  summarize(n_location = n()) %>%
  filter(n_location >= 7)
state_2010
```

    ## # A tibble: 45 × 2
    ##    state n_location
    ##    <chr>      <int>
    ##  1 AL            15
    ##  2 AR            15
    ##  3 AZ            15
    ##  4 CA            60
    ##  5 CO            35
    ##  6 CT            25
    ##  7 DE            15
    ##  8 FL           205
    ##  9 GA            20
    ## 10 HI            20
    ## # … with 35 more rows

In 2002, the states which were observed at 7 or more locations are AZ,
CO, CT, DE, FL, GA, HI, ID, IL, IN, KS, LA, MA, MD, ME, MI, MN, MO, NC,
NE, NH, NJ, NV, NY, OH, OK, OR, PA, RI, SC, SD, TN, TX, UT, VT, WA. In
2010, the states which were observed at 7 or more locations are AL, AR,
AZ, CA, CO, CT, DE, FL, GA, HI, IA, ID, IL, IN, KS, LA, MA, MD, ME, MI,
MN, MO, MS, MT, NC, ND, NE, NH, NJ, NM, NV, NY, OH, OK, OR, PA, RI, SC,
SD, TN, TX, UT, VT, WA, WY.

-   Construct a dataset that is limited to Excellent responses, and
    contains, year, state, and a variable that averages the data\_value
    across locations within a state. Make a “spaghetti” plot of this
    average value over time within a state (that is, make a plot showing
    a line for each state across years – the geom\_line geometry and
    group aesthetic will help).

``` r
excellent_df = brfss %>%
  filter(response == "Excellent") %>%
  group_by(year, state) %>%
  summarize(mean_data_value = mean(data_value))
excellent_df
```

    ## # A tibble: 443 × 3
    ## # Groups:   year [9]
    ##     year state mean_data_value
    ##    <int> <chr>           <dbl>
    ##  1  2002 AK               27.9
    ##  2  2002 AL               18.5
    ##  3  2002 AR               24.1
    ##  4  2002 AZ               24.1
    ##  5  2002 CA               22.7
    ##  6  2002 CO               23.1
    ##  7  2002 CT               29.1
    ##  8  2002 DC               29.3
    ##  9  2002 DE               20.9
    ## 10  2002 FL               25.7
    ## # … with 433 more rows

``` r
excellent_df %>%
  ggplot(aes(x = year, y = mean_data_value)) +
  geom_line(aes(group = state), alpha = .5) +
  labs(
    title = "Average Data Value vs. Year for Each State",
    subtitle = "Only for Excellent Responses",
    x = "Year",
    y = "Average Data Value",
    caption = "Data from brfss_smart2010 dataset"
  ) +
  theme_minimal()
```

<img src="p8105_hw3_wq2161_files/figure-gfm/excellent_df_plot-1.png" width="90%" />

Based on the plot, most of the average data value range from 15 to 30.
The general trend of the average data value for states is decreasing
with time. While there are some states whose average data value is
fluctuated up and down with time.

-   Make a two-panel plot showing, for the years 2006, and 2010,
    distribution of data\_value for responses (“Poor” to “Excellent”)
    among locations in NY State.

``` r
NY_df = brfss %>%
  filter(
    year %in% c(2006, 2010),
    state == "NY"
  ) %>%
  group_by(year, response) %>%
  summarize(data_value)
NY_df
```

    ## # A tibble: 75 × 3
    ## # Groups:   year, response [10]
    ##     year response data_value
    ##    <int> <fct>         <dbl>
    ##  1  2006 Poor            3.3
    ##  2  2006 Poor            3.5
    ##  3  2006 Poor            3.9
    ##  4  2006 Poor            2.3
    ##  5  2006 Poor            2.7
    ##  6  2006 Poor            1.9
    ##  7  2006 Fair           15.3
    ##  8  2006 Fair           11.6
    ##  9  2006 Fair           11.5
    ## 10  2006 Fair           16.3
    ## # … with 65 more rows

``` r
NY_df %>%
  ggplot(aes(x = response, y = data_value, color = year)) +
  geom_point() +
  facet_grid(. ~ year) +
  labs(
    title = "Data Value vs. Responses",
    subtitle = "For NY in Year 2006 and 2010",
    x = "Responses",
    y = "Data Value",
    caption = "Data from brfss_smart2010 dataset"
  ) +
  theme(legend.position = "none")
```

<img src="p8105_hw3_wq2161_files/figure-gfm/NY_df-1.png" width="90%" />

Comment
